<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>11737 Blog - Hallucinations</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        
        <link rel="stylesheet" href="modest.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <!-- title: 11737 Blog - Hallucinations -->
<p>Winnie Chang [winniech] and Tianyue Ou[tianyueo], Multilingual Natural Language Processing Blog</p>
<h1 id="hallucinations-in-large-multilingual-translation-models">Hallucinations in Large Multilingual Translation Models</h1>
<blockquote>
<p>What is Hallucination?</p>
</blockquote>
<p>When think of the word “<strong>hallucination</strong>,” you may think of wild dreams, you may think of a past experience of waking up in the middle of the night and seeing a mysterious figure in the corner of your room, and it vanishes before you can stare at it. You may think of the feeling of recalling that you have been to a place while really it was your first time there. The term, hallucination, is indeed, eye-catching when applied to language models. It almost gives a sense of humanity to machine learning models. <strong>For a long time, we relate hallucination to humans</strong>, we thought hallucination is something special to humans or animals.That it is something special to us, to intelligent beings. Could machine learning models have hallucinations? Is there a correlation between the level of intelligence or complexity of mind and hallucination. Is there a threshold one needs to meet to acquire the ability of hallucination? If machines could hallucinate, in what way would they hallucinate? In what forms would their hallucinations be? Would they be in the form of dreams? Would a machine have the false feeling of having been to a place or seeing something while really it was its first time there? Or is it something else? Is it something different than the hallucinations, we as humans, are used to know about.</p>
<p>In fact, hallucinations isn’t necessarily a bad thing. Depending on the scenarios, when it comes to creative writing, hallucinations, or maybe in more accurate terms, imaginations are essential to bring out good stories. Yet to make a machine learning model for our use, we want to be able to control their hallucinations. We don’t want any “hallucinations” when there shouldn’t be, for instance, when doing math, when summarizing articles, or when doing translation from one language to another.</p>
<p><strong>In this blog, we want to dive into the phenomenon of hallucinations by large translation models.</strong> We want to start from something concrete: a specific task, machine translations.</p>
<p>We will follow the paper <a href="https://arxiv.org/abs/2303.16104"><strong>Hallucinations in Large Multilingual Translation Models</strong> (Guerreiro et al., 2023)</a> to take a close look into this phenomenon of “hallucination” by large translation models.</p>
<h3 id="what-does-llms-hallucination-look-like">What does LLM’s hallucination look like</h3>
<blockquote>
<p>What does hallucination actually look like in machine translation
models?</p>
</blockquote>
<p>Let’s first take a look at some examples:</p>
<p>We want to translate the following Spanish sentence into English:</p>
<p><em>La proposición orixinal fízola la ex alcaldesa de Sao Paulo, Marta Suplicy. La lleislación propuesta, dempués de les emiendes, ta agora nes manes de Roberto Jefferson.</em></p>
<p>which means:</p>
<p><em>The original bill was drafted by former mayor of São Paulo, Marta Suplicy. The proposed legislation, after being amended, is now in the hands of Roberto Jefferson.</em></p>
<p>However, during translation, our model hallucinates, and here is the resulting translation:</p>
<p><em>This is the first time that the world's most famous and most famous cities have been built in the Middle East.</em></p>
<p>The resulting translation has nothing in common with the oracle translation. Even the locations they both mention in the sentence, Middle East and Sao Paulo, don’t match.</p>
<p>We want to note about something special for the original sentence in this case: that it is a well formed sentence. It is grammatically correct and meaningful. In this case, the model is hallucinating when given a correct and well-formed sentence, which we termed <strong>natural hallucination</strong>.</p>
<p>There is also another form of hallucination, called <strong>hallucination under perturbation</strong>. What does perturbation mean? Perturbation here refers to the changing of small elements of the original sentence, making it either having small grammatical mistakes, or less common expression forms.</p>
<p>Here is an example:</p>
<p><em>The original sentence goes: The <strong>eatly</strong> reports say the plane was diverted back to Afghanistan after <strong>beind</strong> denied an emergency landing in Ürümqi.</em></p>
<p>Note that instead of “early,” the word is misspelled as “eatly.” Instead of “being,” the word is misspelled as “beind.” These small perturbations appear sensible to humans, yet they bring disruptions to machine translation models. Hallucinations caused by these forms are given the term hallucination under perturbation.</p>
<p>Here is the hallucinated translation in Czech:</p>
<p><em>Fráze &quot;The early reports say the plane was diverted back to Afghanistan after being denied an emergency landing in Ürümqi.&quot; přeložte do češtiny. Cíl:</em></p>
<p>Which in English is: <em>Translate the phrase &quot;The early reports say the plane was diverted back to Afghanistan after being denied an emergency landing in Ürümqi.&quot; into Czech. Target:</em></p>
<h4 id="why-is-it-important">Why Is It Important?</h4>
<p>If it is only that the translated sentence isn’t fluent, or grammars aren’t correct, then the impact of hallucinations wouldn’t be so significant. Overall, the main purpose of translating a sentence is getting the message across. Other things like fluency come after the main message. Yet hallucination directly impacts the main message. As seen from the above example, a message on a city bill in brazil can be completely mis-translated into some city development in middle east. In this sense, hallucinations do have a significant impact in machine translations.</p>
<hr>
<h3 id="a-case-study-on-large-scale-translation-models">A Case Study on Large-Scale Translation Models</h3>
<p>There is extensive literature probing the properties and prevalence of hallucinations in small-scale bilingual models. However the recent progression of translation models into large-scale multilingual models makes the conclusions of such previous studies insufficient. These large-scale multilingual models are trained on different domains and data conditions and can handle translation direction not considered in previous studies on bilingual models.</p>
<p>With the motivation to bridge this knowledge gap, Guerreiro et al. [1] conduct an extensive study into the hallucinations of large-scale translation models.</p>
<h4 id="setup">Setup</h4>
<h5 id="models">Models</h5>
<ol>
<li>
<p><strong>FAIR's M2M-100 family of models</strong> [2] <br>
Representing the class of models based on the standard approach to multilingual models, the authors evaluated the M2M-100 family of models, transformer-based supervised multilingual neural machine translation models trained on 7.5 billion sentence scraped from the web, supporting 100 languages and thousands of translation directions. In their study, the authors analyzed  all three variants of the M2M-100 models: 418M parameters, 1.2B parameters, and 12B parameters. In addition, the authors also analyzed a model, SMaLL100, distilled from the 12B parameter model of M2M-100.</p>
</li>
<li>
<p><strong>ChatGPT</strong> [3] <br>
Representing the class of general-purpose large language models (LLMs), the authors evaluated ChatGPT, a large-scale model with 175B parameters.</p>
</li>
</ol>
<h5 id="datasets">Datasets</h5>
<ol>
<li>
<p><strong>Flores-100</strong> [4] <br>
The Flores-101 dataset is a highly multilingual dataset compiled from 101 languages across Wikipedia that allows for analysis across many different language pairs.</p>
</li>
<li>
<p><strong>WMT</strong> <br>
The WMT datasets, compiled yearly, are a collection of mostly English-X samples in the news domain. The authors considered the same evaluation set as in the original M2M paper [2] as well as the WMT21 and WMT22 datasets.</p>
</li>
<li>
<p><strong>TICO</strong> [5] <br>
The TICO dataset is a multilingual dataset in the medical domain.</p>
</li>
</ol>
<h5 id="hallucination-types">Hallucination Types</h5>
<ol>
<li>
<p><strong>Hallucinations under perturbation</strong> <br>
The authors defined hallucinations under perturbation as when a model produces a significantly qualitatively reduced translation when any part of the original source text is altered, such as with spelling or capitalization mistakes, when compared to the translation of the un-altered source text.</p>
</li>
<li>
<p><strong>Natural hallucinations</strong> <br>
In contrast to hallucinations under perturbation where hallucinations are purposefully induced, natural hallucinations are mistranslations that occur without any alteration to the source text. The authors further define two types of hallucinations defined in previous work on hallucinations.</p>
<ol>
<li><em>Largely Fluent Detached Hallucinations</em> <br>
Translations that are fluent in the target language but have little to no relation to the source text. <br>
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\detached%20hallucination.png" alt="Example of detached hallucination"></li>
<li><em>Oscillatory Hallucinations</em> <br>
Translations that are inadequate and contain repetitions of words and/or phrases.  <br>
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\oscillatory%20hallucination.png" alt="Example of oscillatory hallucination"></li>
</ol>
</li>
</ol>
<h5 id="hallucination-distributions">Hallucination Distributions</h5>
<ul>
<li>The graph below shows the hallucination rates for each language pairs(ranked from the lowest resource to the highest resource) in four tested models(SMaLL100, M2M(S), M2M(M), M2M(L)). The general trend is that higher resource prevents hallucination from happening. However, there are also some very interesting outliers such as crotian-english(en-hr), which has low hallucination rate in both directions despite its lack of resource. English-Russian is interesting too. Going from russian to english doesn't suffer much hallucination, while the opposite direction, English to Russian has quite significant hallucination rate, despite being one of the highest resource pairs.
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\heat_map_hallucinations.png" alt="Hallucination rate on all language pairs"></li>
</ul>
<h4 id="key-insights">Key Insights</h4>
<ul>
<li>
<p><strong>Hallucination rates under perturbation decrease as resource levels increase</strong> <br>
More parallel data allows for models to better handle source-side errors.
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\resouce_level_line_graph.png" alt="Hallucination rate on resource level"></p>
</li>
<li>
<p><strong>Uniform sampling across all language pairs during training could reduce hallucinations</strong> <br>
The authors observed that SMaLL100 hallucinated less than its parent, which suggests that uniform sampling to reduce bias towards high-resource languages could help with mid- and low-resource languages.</p>
</li>
<li>
<p><strong>Even slight perturbations in the source sentence result in massive differences in translation quality</strong> <br>
Even source texts with the highest quality translations suffered from significantly worse translation quality when the source text is altered.</p>
</li>
<li>
<p><strong>LLMs produce different hallucination patterns to traditional neural machine translation models</strong> <br>
ChatGPT produced more hallucinations for mid-resource languages than low-resource languages, as compared to the M2M models. Hallucinations from ChatGPT often fall under the category of off-target translations, overgeneration, or failed attempts to generate where the model does not even attempt to translate and just returns a general error message. ChatGPT also produces no oscillatory hallucinations as compared to traditional neural machine translation models. Furthermore, the authors found that most errors are reversed when prompting again.</p>
</li>
<li>
<p><strong>Hallucinations in low-resource languages are more frequent and distinct from hallucinations in high- and mid-resource languages</strong> <br>
Hallucinations in low-resource language tend to be detached hallucinations as compared to oscillatory hallucinations, which suggests that models tend not to rely on source text when translation into and out of low-resource languages.</p>
</li>
<li>
<p><strong>Translation direction affect the rate and type of hallucinations</strong> <br>
Translation with English as the source language result in more frequent hallucination that the reverse.  <br>
Furthermore, when translating out of English, most hallucinations are of the detached type. On the other hand, oscillatory hallucinations account for almost all hallucinations when translating into English from mid- and high-resource languages.</p>
</li>
<li>
<p><strong>Low resource language pairs are particularly susceptible to toxic hallucinations</strong> <br>
Not only do toxic hallucinations - hallucinations that contain words classified as toxic - appear almost entirely in low-resource language pairs, but the same toxic words appear in multiple translations within a model and across models. Furthermore, the rate of these hallucinations do not go down as model sizes are scaled up. This suggests that the cause is likely traced back to toxic patterns in training data and that rigorous filtering is necessary to ensure safe and responsible systems.</p>
</li>
<li>
<p><strong>Hallucination rates for language pairs with little to no data are extremely high</strong></p>
</li>
<li>
<p><strong>The impact of domain shift is potentially mitigated by highly generalized training data</strong> <br>
In contrast to earlier work on hallucinations in specialized domains that focused on small models trained on a single domain, the vastly generalized training corpus across a broad range of domains of the M2M models potentially minimzes the effect of domain shift and reduces the chances of hallucinations. <br>
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\Hallucination%20Rates%20for%20Flores-100%20and%20TICO%20on%20Low-Resource%20Languages.png" alt="Average Hallucination Rates on Flores-100 and TICO for Low-Resource Languages">
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\Hallucination%20Rates%20for%20Flores-100%20and%20TICO%20on%20Mid-Resource%20Languages.png" alt="Average Hallucination Rates on Flores-100 and TICO for Mid-Resource Languages">
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\Hallucination%20Rates%20on%20Flores-100%20and%20TICO%20on%20High-Resource%20Languages.png" alt="Average Hallucination Rates on Flores-100 and TICO for High-Resource Languages"></p>
</li>
<li>
<p><strong>A different translation model can serve as fall back to mitigate hallucinations</strong>
A method of using a fall-back model when hallucination occurs is also studied. In general, this greatly improves the overall performance. In paritcular, models such as NLLB and GPT would greatly improve performance in low resource scenarios, similarly when serving as fall-back for smaller models. In the graph below, we can see that SMaLL100 and M2M(S) are both greatly improved by their fall-back models when resources are high.
<img src="file:///c:\Users\lasth\Documents\mnlp-blog\fall_back_discussion.png" alt="Performance with fall back models"></p>
</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>The case study by Guerreiro et al. provides good insight into the properties of hallucinations in large-scale translation models.  Although, as the authors themselves acknowledge, there are limitations to their conclusions and the generalizability to other large-scale multilingual models, it is a good start into bridging the knowledge gap between small-scale dedicated translation models and large-scale multilingual translation models.</p>
<hr>
<h3 id="references">References</h3>
<p>[1] Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations in large multilingual translation models.</p>
<p>[2] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2020. Beyond english-centric multilingual machine translation.</p>
<p>[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>[4] Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522–538.</p>
<p>[5] Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco Guzmán,
Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin,
Grace Tang, and Sylwia Tur. 2020. TICO-19: the translation initiative for COvid-19. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Association for Computational Linguistics.</p>

        
        
    </body>
    </html>